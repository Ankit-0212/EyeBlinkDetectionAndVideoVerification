During real-time video conferences, it's important to be able to tell the difference between still images and looping
videos so that everyone pays attention and helps reach the meeting's goals. When remote work and virtual meetings
grow in popularity, individuals are more likely to turn off their cameras or watch pre-recorded content to avoid
speaking to one another. In real-time video conferences, when static photographs and repeating videos are
recognized, organizers can intervene and encourage participation to ensure that everyone's ideas are heard, attention
and concentration are maintained, and sensible decisions are made. The study shows that the proposed model for
categorizing data works well and can be used in real-world situations. A prediction accuracy score of 1 indicates that
the model correctly predicted all of the samples in the dataset. A recall score of 0.994% means that 99.4% of the
positive samples in the dataset were correctly identified by the model. This shows that the model is very sensitive to
positive samples. The F1 score of 0.997 demonstrates an excellent mix of precision and recall, with few false
positives and false negatives. With an average squared error of 0.0205, the model's predictions are likely accurate.
Although the metrics tell us a great deal about the performance of the model, we must remember that they do not tell
us everything. You should also examine the quality of the data, the complexity of the model, and how easy it is to
interpret. But the results suggest that the model could be useful for real-world classification problems. For meetings
to go well and get things done, the students need to be able to tell the difference between still images and looping
videos. The proposed categorization model has shown that it can classify things in an effective and sensitive way.
This means that it could be used in the real world.

The main goal is to look at possible outcomes using graphs and
numbers. The basic premise of this study is that there is a distinction between the upper and lower eyelids when the
iris of an eye is observed. The marks p1, p2, p3, and p4 mark the upper eyelid, while the points p1, p6, p5, and p4
mark the lower eyelid. When the eyelids are closed, there is no gap between the eyes, and p2, p6, and p3, p5 overlap.
This enabled the observation of a blink. Figure 2 depicts how a blink is captured. The suggested model calculates the
F1 score, MSE score, and precision score based on the blinking value. Recall the hour of execution and other details.
Precision is the model's capacity to correctly recognize positive samples. Because the accuracy score in this instance
is 1, all positive samples are present. This is a favorable outcome since it demonstrates that the model correctly
interprets all negative input as positive. Notably, a model may not always have an accuracy score of 1, as it may
ignore all positive examples. In contrast, the recall score quantifies the model's capacity to locate all positive
samples. In this instance, the recall is 0.994%, which indicates that the model correctly detects positive samples
99.4% of the time. This is a beneficial effect because it demonstrates that the model lacks an abundance of success
stories. Note that a recall score of 0.994% indicates that the model lacks some positive examples. Depending on the
situation and what will happen if positive samples are lost, this may or may not be okay. The F1 score is the
harmonic mean of the accuracy and recall of the model. It provides a single measure of the model's accuracy that
includes both precision and recall.
